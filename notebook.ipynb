{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Załadowanie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = Path(f\"{os.getcwd()}\\\\main_data.json\")\n",
    "if not os.path.isfile(FILE):\n",
    "    raise FileNotFoundError(\"Nie odnaleziono pliku!\")\n",
    "else:\n",
    "    print(f\"Plik {FILE} już jest na dysku\")\n",
    "\n",
    "data = pd.read_json(FILE)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane są prawie pełne, w dwóch meczach brakuje statystyk odnośnie drużyny B. Pozwolę sobie je usunąć, gdyż 2 w mecze w kontekście prawie 8000 nie wpłyną na wyniki analizy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacja danych (rozwinięcie słownika na kolumny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dictionary_in_dataframe(frame: pd.DataFrame, column_name: str):\n",
    "    frame[column_name].apply(pd.Series)\n",
    "    expanded_cols = frame[column_name].apply(pd.Series)\n",
    "    new_names = {col: f\"{column_name[0]}_{col}\" for col in expanded_cols.columns}\n",
    "    expanded_cols = expanded_cols.rename(columns=new_names)\n",
    "    frame = frame.drop(column_name, axis=1)\n",
    "    return frame.join(expanded_cols)\n",
    "\n",
    "\n",
    "# wydarzył się jeden mecz na mapie Train, nie będziemy brać tego meczu pod uwagę\n",
    "trains_matches = data[(data['map'] == \"Train\")].index\n",
    "data = data.drop(trains_matches)\n",
    "\n",
    "print(set(data['map']))\n",
    "data = expand_dictionary_in_dataframe(data, \"A_stats\")\n",
    "data = expand_dictionary_in_dataframe(data, \"B_stats\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usunięcie kolumn z nazwami drużyn i ID meczu - nie są potrzebne do analizy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"team_A\", axis=1)\n",
    "data = data.drop(\"team_B\", axis=1)\n",
    "data = data.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowe nazwy dla rund zdobtych przez drużyny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'team_A_score' : 'A_score', 'team_B_score' : 'B_score'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodanie kolumn o tym kto wygrał mecz. Dodam kolumnę 'Winner' gdzie będą mogły się znajdować 2 wartości: A, B. Nie ma możliwości zakończenia się meczu remisem. Pózniej wykorzystam Ordinal encoder do zamiany wartości na wartość numeryczną<br>\n",
    "|Condition|Winner|\n",
    "|---|---|\n",
    "|A_score $\\gt$ B_score| A|\n",
    "|A_score $\\lt$ B_score| B|\n",
    "\n",
    "Oraz dodanie tieru drużyny na podstawie rankingu:\n",
    "|Condition|Tier|\n",
    "|---|---|\n",
    "|rank $\\leq$ 20|1|\n",
    "|20 $\\lt$ rank $\\leq$ 80|2|\n",
    "|80 $\\lt$ rank|3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"A tier\"] = data[\"A_rank\"].apply(lambda x: 1 if x <= 20 else (2 if x <= 80 else 3))\n",
    "data[\"B tier\"] = data[\"B_rank\"].apply(lambda x: 1 if x <= 20 else (2 if x <= 80 else 3))\n",
    "data[\"Winner\"] = np.where(data[\"A_score\"] > data[\"B_score\"], \"A\", \"B\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstępna analiza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozkład liczby meczów na pozsczególnych mapach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data, hue='map', x='map')\n",
    "plt.tight_layout()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapa Dust2 odstaje, gdyż w ostatnim czasie zmienił się map pool, za Overpassa wszedł właśnie Dust2 i nie rozegrano na nim jeszcze wiele meczów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zakres statystyk drużyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Team A and Team B data (assuming separate columns)\n",
    "team_a_data = data[[col for col in data.columns if col.startswith('A_')]]\n",
    "team_b_data = data[[col for col in data.columns if col.startswith('B_')]]\n",
    "\n",
    "for feature_A, feature_B in zip(team_a_data.columns, team_b_data.columns):\n",
    "    # Extract data and labels\n",
    "    team_a_values = team_a_data[feature_A]\n",
    "    team_b_values = team_b_data[feature_B]\n",
    "    feature_name = feature_A[2:]  # Remove 'A_' prefix\n",
    "\n",
    "    # Create a box plot\n",
    "    plt.figure()\n",
    "    plt.boxplot([team_a_values, team_b_values], labels=['Team A', 'Team B'], notch=True)\n",
    "    plt.title(f\"{feature_name} comparison\")\n",
    "\n",
    "    plt.axhline(y=np.mean(team_a_data[feature_A]), color='red', linestyle='dashed', label=f'Team A Mean ({np.mean(team_a_data[feature_A]):.2f})')\n",
    "    plt.axhline(y=np.mean(team_b_data[feature_B]), color='blue', linestyle='dashed', label=f'Team B Mean ({np.mean(team_b_data[feature_B]):.2f})')\n",
    "\n",
    "    plt.xlabel('Team')\n",
    "    plt.ylabel(feature_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla wartości, które bezpośrednio zależą od liczby rozegranych rund, takich jak kille, asysty, headshoty, śmierci, wygrane rundy, jest znacznie więcej lierów niż dla wartości, które są niezależne od liczby rund, tj. KAST, Rating, FK_Diff, ADR. <br>\n",
    "Jednak tych wartości odstających nie musimy usuwać, gdyż są one związane z liczbą rozegranych rund i są zależne liniowo od tej liczby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie zależności między wygranym meczem, a tierami drużyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matches_by_team_tier(tier1: int, tier2: int):\n",
    "    \"\"\"\n",
    "    Returns matches where teams are from a given tiers.\n",
    "    \"\"\"\n",
    "    matchups = (data[\"A tier\"] == tier1) & (data[\"B tier\"] == tier2)\n",
    "    return data[matchups]\n",
    "\n",
    "\n",
    "for tiers in np.array(np.meshgrid([1, 2, 3], [1, 2, 3])).T.reshape(-1, 2):\n",
    "    sns.countplot(\n",
    "        x=\"Winner\",\n",
    "        data=filter_matches_by_team_tier(*tiers),\n",
    "        hue=\"Winner\",\n",
    "        stat=\"proportion\",\n",
    "    )\n",
    "    plt.title(f\"Comparison: A tier {tiers[0]} vs B tier {tiers[1]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można zauważyć, że jeśli spotykają się drużyny z tego samego tieru to szansa na wygranie jednej z drużyn wynosi około 50%. Szansa ta rośnie jeśli różnica między tierami rośnie (drużyna z lepszego tieru ma większe szanse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pórwnanie wpływu mapy na wynik meczu, w zależności od tieru drużyny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tiers in [(1, 2), (2, 1), (1, 3), (3, 1), (2, 3), (3, 2)]:\n",
    "    sns.countplot(\n",
    "        x=\"map\",\n",
    "        data=filter_matches_by_team_tier(*tiers),\n",
    "        hue=\"Winner\",\n",
    "    ).set_xlabel(\"Winner on map\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.title(f\"Comparison: A tier {tiers[0]} vs B tier {tiers[1]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z powyższych wykresów można wywnioskować, że jesli grają drużyny z 1 i 2 tieru, to na Anubisie i Inferno drużyna z 1 tieru ma większe szanse.<br>\n",
    "Sytuacja się zmienia jeśli 1 tier gra z tierem 3, wtedy Ancient jest najlepszym wyborem dla drużyny z tieru 1.<br>\n",
    "Jesli chodzi o tier 2 i 3, to Ancient, Anubis oraz Nuke, zwiększałyby szanse na wygraną dla drużyny z tieru 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enkodowanie cech kategorycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enkodowanie wartości map przy użyciu OrdinalEncodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(dtype=int)\n",
    "encoder.fit(data[[\"map\", \"Winner\"]])\n",
    "data[[\"map\", \"Winner\"]] = encoder.transform(data[[\"map\", \"Winner\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych do trenowania modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodzielenie cech od wartości do predykowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, features: list[str]):\n",
    "    results_cols = [\"Winner\"]\n",
    "    return dataset[features], dataset[results_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skalowanie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(dataset, scaler):\n",
    "        return scaler.fit_transform(dataset.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "Z racji iż występuje nierównowaga co do liczności klas postanowiłem je wyrównać, na \"surowych danych\" model znacząco częściej predykował dominującą klasę.<br>\n",
    "Po downsamplingu zaczął bardziej równomiernie przewidywać wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_distribution = data['Winner'].value_counts()\n",
    "print(winner_distribution)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = data[data['Winner'] == 0]\n",
    "df_minority = data[data['Winner'] == 1]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=len(df_minority),     # to match minority class\n",
    "                                   random_state=123) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "data_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Display new class counts\n",
    "print(data_downsampled['Winner'].value_counts())\n",
    "data_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór najlepszych parametrów do modelu za pomocą GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_parameters(model, parameters, X, y, cv=10, n_jobs=-1):\n",
    "    grid_object = GridSearchCV(\n",
    "        model,\n",
    "        parameters,\n",
    "        scoring=make_scorer(precision_score),\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    grid_object = grid_object.fit(X, y)\n",
    "    return grid_object.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzenie modeli na różnych cechach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    data.columns.delete([1, 2, len(data.columns) - 1]),\n",
    "    [\"A_score\", \"B_score\"],\n",
    "    [\"A_rank\", \"B_rank\"],\n",
    "    [\"A_rank\", \"B_rank\", \"A tier\", \"B tier\"],\n",
    "    [\"A_AVG_K\", \"A_AVG_D\", \"B_AVG_K\", \"B_AVG_D\"],\n",
    "    [\"A_AVG_ADR\", \"B_AVG_ADR\"],\n",
    "    [\"A_AVG_KAST\", \"B_AVG_KAST\"],\n",
    "    [\"A_AVG_FK Diff\", \"B_AVG_FK Diff\"],\n",
    "    [\"A_AVG_Rating2.0\", \"B_AVG_Rating2.0\"],\n",
    "    [\"map\", \"A tier\", \"B tier\"],\n",
    "]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_of_features in FEATURES:\n",
    "    X, Y = split_dataset(data_downsampled, set_of_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, Y, train_size=0.75, random_state=42, shuffle=True\n",
    "    )\n",
    "    y_train = y_train.values.ravel()\n",
    "    y_test = y_test.values.ravel()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scale_features(X_train, scaler)\n",
    "    X_test = scale_features(X_test, scaler)\n",
    "\n",
    "    clf = SVC()\n",
    "\n",
    "    parameters = {\"C\": [1.0, 2.0, 4.0], \"gamma\": [0.001, 0.1, 1.0, 10.0]}\n",
    "\n",
    "    clf = find_best_parameters(clf, parameters, X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[clf.classes_])\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=[\"A\", \"B\"],\n",
    "    )\n",
    "    disp.plot()\n",
    "\n",
    "    str_features = \"\".join(f\"{x}, \" for x in set_of_features)\n",
    "    plt.title(f\"Features: {str_features}\", wrap=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    results.append(\n",
    "        [\n",
    "            str_features,\n",
    "            np.mean(cross_val_score(clf, X_train, y_train, scoring=\"precision\", cv=10)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "pd.DataFrame(results, columns=[\"Features\", \"Precision\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
